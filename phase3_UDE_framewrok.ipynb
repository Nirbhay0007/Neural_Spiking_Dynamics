{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SciMLSensitivity\n",
    "using DifferentialEquations\n",
    "using SciMLSensitivity   # or DiffEqSensitivity if you prefer\n",
    "using Zygote\n",
    "using Optimisers         # for optimizer & update\n",
    "using LinearAlgebra\n",
    "using DifferentialEquations\n",
    "using Flux\n",
    "using Plots\n",
    "using Optimization\n",
    "using OptimizationOptimisers\n",
    "using Zygote\n",
    "using DataFrames\n",
    "\n",
    "using Random\n",
    "Random.seed!(1234)\n",
    "println(\"All the nessecessary packages have been imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hodgkin-Huxley Model Parameters (Global Constants)\n",
    "\n",
    "\n",
    "# Physical Constants\n",
    "const Cm = 1.0        # ŒºF/cm^2\n",
    "const g_Na = 120.0    # mS/cm^2\n",
    "const g_K = 36.0      # mS/cm^2\n",
    "const g_L = 0.3       # mS/cm^2\n",
    "const E_Na = 50.0     # mV\n",
    "const E_K = -77.0     # mV\n",
    "const E_L = -54.387   # mV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Known Physics & Stimulus ---\n",
    "\n",
    "# Voltage-gated ion channel kinetics\n",
    "Œ±_n(V) = 0.01 * (V + 55) / (1 - exp(-(V + 55) / 10))\n",
    "Œ≤_n(V) = 0.125 * exp(-(V + 65) / 80)\n",
    "Œ±_m(V) = 0.1 * (V + 40) / (1 - exp(-(V + 40) / 10))\n",
    "Œ≤_m(V) = 4.0 * exp(-(V + 65) / 18)\n",
    "Œ±_h(V) = 0.07 * exp(-(V + 65) / 20)\n",
    "Œ≤_h(V) = 1 / (1 + exp(-(V + 35) / 10))\n",
    "\n",
    "# Steady-state & time-constant functions for the 2D model\n",
    "m_inf(V) = Œ±_m(V) / (Œ±_m(V) + Œ≤_m(V))\n",
    "h_inf(V) = Œ±_h(V) / (Œ±_h(V) + Œ≤_h(V))\n",
    "n_inf(V) = Œ±_n(V) / (Œ±_n(V) + Œ≤_n(V))\n",
    "tau_n(V) = 1 / (Œ±_n(V) + Œ≤_n(V))\n",
    "\n",
    "println(\"Physics of neural dynamics has been defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ab11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Stimulus(t)\n",
    "    # A 1ms pulse starting at 10ms\n",
    "    return(t>-10.0 && t<11.0) ? 20 : 0.0\n",
    "end\n",
    "\n",
    "println(\" An extra current form neighbour to generate a pulse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Data Generation ---\n",
    "\n",
    "# 2D Hodgkin-Huxley reduced model engine\n",
    "function hodgkin_huxley_reduced!(du, u, p, t)\n",
    "    V, n = u\n",
    "    I_ext = Stimulus(t)\n",
    "\n",
    "\n",
    "    # Known 2D current\n",
    "    I_Na = g_Na * m_inf(V)^3 * h_inf(V) * (V - E_Na)\n",
    "    I_K  = g_K  * n^4 * (V - E_K)\n",
    "    I_L  = g_L * (V - E_L)\n",
    "    du[1] = (I_ext - I_Na - I_K - I_L) / Cm\n",
    "    du[2] = (n_inf(V) - n) / tau_n(V)\n",
    "end\n",
    "\n",
    "# Generate Data\n",
    "u0_true = [-65.0, n_inf(-65.0)]\n",
    "tspan = (0.0, 50.0)\n",
    "prob_true = ODEProblem(hodgkin_huxley_reduced!, u0_true, tspan)\n",
    "sol_true = solve(prob_true, Rodas5P(), saveat=0.5)\n",
    "\n",
    "# Extract and structure the training data\n",
    "data_V = sol_true[1, :]\n",
    "t_train = sol_true.t\n",
    "\n",
    "# (Optional) Verify data shape and content\n",
    "df = DataFrame(t=t_train, V=data_V )\n",
    "println(\"Generated Training Data:\")\n",
    "display(first(df, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aba3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = Chain(\n",
    "    Dense(1,15, tanh,init = Flux.glorot_uniform),\n",
    "    # Dense(15,30,tanh,init = Flux.glorot_uniform),\n",
    "    Dense(15,1,init=Flux.glorot_uniform)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25341977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the trainable parameters (p_nn) and the re-structuring function (re)\n",
    "p_nn, re = Flux.destructure(U)\n",
    "println(\"Recruit Constructed. Parameters: \", length(p_nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb12fe",
   "metadata": {},
   "source": [
    "## The hybrid UDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff65eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the UDE function with the embedded neural network\n",
    "function ude_dynamics!(du, u, p, t)\n",
    "    V, n = u\n",
    "    # p --> p_nn neural network parameters\n",
    "    # Neural network component to learn the unknown current\n",
    "    # we will normalize V roughly ( divide by 100) to keep inputs clean for the NN\n",
    "\n",
    "    nn_input = V / 100.0\n",
    "    # We divide V by 100.0 to keep inputs small for the Neural Network\n",
    "    # Example: -65mV becomes -0.65\n",
    "\n",
    "    nn_I_Na = re(p)([nn_input])[1]\n",
    "    # Known physics components\n",
    "    I_ext = Stimulus(t)\n",
    "    I_K  = g_K  * n^4 * (V - E_K)\n",
    "    I_L  = g_L * (V - E_L)\n",
    "    \n",
    "    # The hybrid dynamics equation\n",
    "    du[1] = (I_ext + nn_I_Na - I_K - I_L) / Cm\n",
    "    du[2] = (n_inf(V) - n) / tau_n(V)\n",
    "end\n",
    "println(\"Hybrid Engine Assembled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Stable predict function using BacksolveAdjoint and Float64 inputs ----\n",
    "\n",
    "\n",
    "\n",
    "prob_nn = ODEProblem(ude_dynamics!,u0_true,tspan , p_nn)\n",
    "function predict_ude(p)\n",
    "    # build problem with the current flattened NN params\n",
    "\n",
    "\n",
    "    _prob=remake(prob_nn,p=p)\n",
    "    \n",
    "    \n",
    "    solve(_prob, Rodas5P(), saveat=t_train, \n",
    "          sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()))\n",
    "end\n",
    "\n",
    "\n",
    "# ---- Loss function (keep as Float64) ----\n",
    "function loss(p)\n",
    "    pred = predict_ude(p)\n",
    "    if pred.retcode != :Success\n",
    "        return 1e6\n",
    "    end\n",
    "    pred_V = pred[1, :]\n",
    "    loss_val = sum(abs2, pred_V .- data_V)\n",
    "    return loss_val\n",
    "end\n",
    "\n",
    "println(\"Objective Functions Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using JLD2\n",
    "const CHECKPOINT_FILE = \"neuron_mission_log.jld\" # Corrected the typo from CKECKPOINT_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e127ab",
   "metadata": {},
   "source": [
    "#### Creating a `callback()`\n",
    "a callback() function to measure the loss , store the loss and continue from the stoppez "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container to hold all losses in memory for plotting later\n",
    "all_losses = Float64[]\n",
    "\n",
    "# We create a robust callback generator\n",
    "function create_callback(phase_name)\n",
    "    return function (p, l)\n",
    "        # 1. Check for failure (Exploding Gradients)\n",
    "        if isnan(l)\n",
    "            @warn \"!!! ABORT MISSION: Loss is NaN in $phase_name !!!\"\n",
    "            return true # halting the optimization\n",
    "        end\n",
    "        \n",
    "        # 2. Record the loss\n",
    "        push!(all_losses, l)\n",
    "        \n",
    "        # 3. SAVE TO DISK (The Checkpoint)\n",
    "        # We save the current parameters 'p' and the history 'all_losses'\n",
    "        # Saving every iteration on a small model (46 params) is fast and safe.\n",
    "        jldsave(CHECKPOINT_FILE; params=p, loss_history=all_losses)\n",
    "\n",
    "        # 4. Status Report (Every 50 steps)\n",
    "        current_iter = length(all_losses)\n",
    "        if current_iter % 50 == 0\n",
    "            println(\"[$phase_name] Iter: $current_iter | Loss: $l\")\n",
    "        end\n",
    "        \n",
    "        return false # Continue training\n",
    "    end\n",
    "end\n",
    "\n",
    "# Resume Function: Checks if we have previous intel\n",
    "function load_checkpoint_if_exists(initial_params)\n",
    "    if isfile(CHECKPOINT_FILE)\n",
    "        println(\"üìÇ INTEL FOUND: Loading previous checkpoint...\")\n",
    "        data = load(CHECKPOINT_FILE)\n",
    "        # Restore global history\n",
    "        global all_losses = data[\"loss_history\"]\n",
    "        # Return saved parameters\n",
    "        println(\"   -> Resuming from Iteration $(length(all_losses)) with Loss $(last(all_losses))\")\n",
    "        return data[\"params\"]\n",
    "    else\n",
    "        println(\"üåü NO INTEL: Starting fresh recruit training.\")\n",
    "        return initial_params\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Functions Defined.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizationFunction{true, AutoZygote, var\"#27#28\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}(var\"#27#28\"(), AutoZygote(), nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, SciMLBase.DEFAULT_OBSERVED_NO_TIME, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss(p)\n",
    "    pred = predict_ude(p)\n",
    "    if pred.retcode != :Success\n",
    "        return 1e6\n",
    "    end\n",
    "    pred_V = pred[1, :]\n",
    "    loss_val = sum(abs2, pred_V .- data_V)\n",
    "    return loss_val\n",
    "end\n",
    "\n",
    "println(\"Objective Functions Defined.\")\n",
    "\n",
    "# Define the OptimizationFunction\n",
    "optf = Optimization.OptimizationFunction((p, adtype) -> loss(p), Optimization.AutoZygote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü NO INTEL: Starting fresh recruit training.\n",
      "\n",
      "--- STAGE 1: ROUGH MANEUVERS (Adam 0.05) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Warning: Layer with Float32 parameters got Float64 input.\n",
      "‚îÇ   The input will be converted, but any earlier layers may be very slow.\n",
      "‚îÇ   layer = Dense(1 => 15, tanh)\n",
      "‚îÇ   summary(x) = 1-element Vector{Float64}\n",
      "‚îî @ Flux C:\\Users\\nirbh\\.julia\\packages\\Flux\\uRn8o\\src\\layers\\stateless.jl:60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage1] Iter: 50 | Loss: 2604.732604184652\n",
      "[Stage1] Iter: 100 | Loss: 1761.3410719238284\n",
      "[Stage1] Iter: 150 | Loss: 1557.4089777287425\n",
      "[Stage1] Iter: 200 | Loss: 1314.7096864810308\n",
      "[Stage1] Iter: 250 | Loss: 991.2153812671663\n",
      "[Stage1] Iter: 300 | Loss: 754.2350997330925\n",
      "[Stage1] Iter: 350 | Loss: 680.12886148631\n",
      "[Stage1] Iter: 400 | Loss: 668.3740903523076\n",
      "[Stage1] Iter: 450 | Loss: 636.6761430952537\n",
      "[Stage1] Iter: 500 | Loss: 613.9090528627605\n",
      "[Stage1] Iter: 550 | Loss: 593.9281586054842\n",
      "[Stage1] Iter: 600 | Loss: 576.4011893335945\n",
      "[Stage1] Iter: 650 | Loss: 561.1952538639416\n",
      "[Stage1] Iter: 700 | Loss: 550.9160434889776\n",
      "[Stage1] Iter: 750 | Loss: 534.6316304026655\n",
      "[Stage1] Iter: 800 | Loss: 516.4121136681152\n",
      "[Stage1] Iter: 850 | Loss: 499.91727665554146\n",
      "[Stage1] Iter: 900 | Loss: 506.9220221127258\n",
      "[Stage1] Iter: 950 | Loss: 427.91116827945206\n",
      "[Stage1] Iter: 1000 | Loss: 263.0439166032882\n",
      "\n",
      "--- STAGE 2: FINE TUNING (Adam 0.01) ---\n",
      "[Stage2] Iter: 1050 | Loss: 187.11952826689017\n",
      "[Stage2] Iter: 1100 | Loss: 137.89403067137155\n",
      "[Stage2] Iter: 1150 | Loss: 111.27185124200336\n",
      "[Stage2] Iter: 1200 | Loss: 86.47032398809307\n",
      "[Stage2] Iter: 1250 | Loss: 66.66885756547056\n",
      "[Stage2] Iter: 1300 | Loss: 55.53821072572243\n",
      "[Stage2] Iter: 1350 | Loss: 48.01596271926509\n",
      "[Stage2] Iter: 1400 | Loss: 40.82144154216564\n",
      "[Stage2] Iter: 1450 | Loss: 31.805859539093515\n",
      "[Stage2] Iter: 1500 | Loss: 27.4600775039447\n",
      "[Stage2] Iter: 1550 | Loss: 24.324919781946214\n",
      "[Stage2] Iter: 1600 | Loss: 21.012041312737683\n",
      "[Stage2] Iter: 1650 | Loss: 18.866339881886912\n",
      "[Stage2] Iter: 1700 | Loss: 17.113074323580552\n",
      "[Stage2] Iter: 1750 | Loss: 15.718745407513984\n",
      "[Stage2] Iter: 1800 | Loss: 14.147877485100034\n",
      "[Stage2] Iter: 1850 | Loss: 13.173302715298746\n",
      "[Stage2] Iter: 1900 | Loss: 12.275417359841853\n",
      "[Stage2] Iter: 1950 | Loss: 11.458567742780332\n",
      "[Stage2] Iter: 2000 | Loss: 10.740480092527912\n",
      "\n",
      "--- STAGE 3: THE POLISH (L-BFGS) ---\n",
      "‚ö†Ô∏è L-BFGS encountered instability or interrupt. Parameters saved safely.\n",
      "Error: UndefVarError(:OptimizationOptimJL, Main)\n",
      "\n",
      "üéâ MISSION COMPLETE. Total Iterations: 2001\n",
      "   Final Saved Loss: 10.740480092527912\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD STARTING STATE ---\n",
    "# Check if we crashed before. If so, resume from there.\n",
    "# If not, start with fresh recruit parameters (p_nn).\n",
    "current_params = load_checkpoint_if_exists(p_nn)\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 1: ROUGH TRAINING (Adam lr=0.05)\n",
    "# Goal: Quickly find the general shape of the valley\n",
    "# ==========================================\n",
    "if length(all_losses) < 1000\n",
    "    println(\"\\n--- STAGE 1: ROUGH MANEUVERS (Adam 0.05) ---\")\n",
    "    optprob = Optimization.OptimizationProblem(optf, current_params)\n",
    "    \n",
    "    res1 = Optimization.solve(optprob, OptimizationOptimisers.Adam(0.05), \n",
    "                              callback=create_callback(\"Stage1\"), \n",
    "                              maxiters=1000 - length(all_losses)) # Only run what's left\n",
    "    \n",
    "    global current_params = res1.u # Update our best params\n",
    "else\n",
    "    println(\"‚úÖ Stage 1 completed previously.\")\n",
    "end\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 2: FINE TUNING (Adam lr=0.01)\n",
    "# Goal: Settle into the stable orbit (limit cycle/spike)\n",
    "# ==========================================\n",
    "if length(all_losses) < 2000\n",
    "    println(\"\\n--- STAGE 2: FINE TUNING (Adam 0.01) ---\")\n",
    "    optprob2 = Optimization.OptimizationProblem(optf, current_params)\n",
    "    \n",
    "    # Calculate remaining iterations needed to reach 2000\n",
    "    remaining_iters = 2000 - length(all_losses)\n",
    "    \n",
    "    if remaining_iters > 0\n",
    "        res2 = Optimization.solve(optprob2, OptimizationOptimisers.Adam(0.01), \n",
    "                                  callback=create_callback(\"Stage2\"), \n",
    "                                  maxiters=remaining_iters)\n",
    "        global current_params = res2.u\n",
    "    end\n",
    "else\n",
    "    println(\"‚úÖ Stage 2 completed previously.\")\n",
    "end\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 3: FINAL POLISH (L-BFGS)\n",
    "# Goal: Mathematical precision. \n",
    "# Note: L-BFGS requires `OptimizationOptimJL` and can result in NaNs if unstable.\n",
    "# ==========================================\n",
    "println(\"\\n--- STAGE 3: THE POLISH (L-BFGS) ---\")\n",
    "\n",
    "# Checkpointing L-BFGS is harder because it keeps internal memory (Hessian).\n",
    "# We essentially restart L-BFGS from our best point every time we run this block.\n",
    "optprob3 = Optimization.OptimizationProblem(optf, current_params)\n",
    "\n",
    "try\n",
    "    res3 = Optimization.solve(optprob3, OptimizationOptimJL.LBFGS(), \n",
    "                              callback=create_callback(\"Stage3_LBFGS\"), \n",
    "                              maxiters=500) # Run for 500 steps of polishing\n",
    "    global current_params = res3.u\n",
    "catch e\n",
    "    println(\"‚ö†Ô∏è L-BFGS encountered instability or interrupt. Parameters saved safely.\")\n",
    "    println(\"Error: \", e)\n",
    "end\n",
    "\n",
    "println(\"\\nüéâ MISSION COMPLETE. Total Iterations: $(length(all_losses))\")\n",
    "println(\"   Final Saved Loss: $(last(all_losses))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0845c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `losses` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `losses` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ e:\\Neural_Spiking_Dynamics\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X20sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "plot(losses,\n",
    "     xlabel=\"Iteration\",\n",
    "     ylabel=\"Loss\",\n",
    "     title=\"Training Loss (Linear Scale)\",\n",
    "     label=\"Loss\",\n",
    "     lw=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ffaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `res` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `res` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ e:\\Neural_Spiking_Dynamics\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X21sZmlsZQ==.jl:3"
     ]
    }
   ],
   "source": [
    "# 2. Visualizing the Recruit vs The Master\n",
    "# Run a prediction with the TRAINED parameters (res.u)\n",
    "final_sol = predict_ude(res.u)\n",
    "\n",
    "p2 = plot(t_train, data_V, label=\"Ground Truth\", lw=3, alpha=0.5, color=:green)\n",
    "plot!(p2, final_sol.t, final_sol[1,:], label=\"UDE Prediction\", lw=2, color=:red, linestyle=:dash)\n",
    "title!(p2, \"Neural Network Performance\")\n",
    "xlabel!(\"Time (ms)\")\n",
    "ylabel!(\"Voltage (mV)\")\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815daa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `losses_adam` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `losses_adam` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ e:\\Neural_Spiking_Dynamics\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X22sZmlsZQ==.jl:2"
     ]
    }
   ],
   "source": [
    "# Combined Plotting\n",
    "total_iterations = 1:(length(losses_adam) + length(losses_lbfgs))\n",
    "\n",
    "# Setup the canvas\n",
    "p_combined = plot(title=\"Dual-Phase Training (Adam -> L-BFGS)\", \n",
    "                  xlabel=\"Iteration\", ylabel=\"Loss (Log Scale)\", yaxis=:log)\n",
    "\n",
    "# Plot Phase 1: Adam (Blue)\n",
    "plot!(p_combined, 1:length(losses_adam), losses_adam, \n",
    "      label=\"Phase I: Adam (Coarse)\", color=:blue, lw=2)\n",
    "\n",
    "# Plot Phase 2: L-BFGS (Red)\n",
    "# We shift the x-axis so it starts exactly where Adam ended\n",
    "range_phase2 = (length(losses_adam)+1):length(total_iterations)\n",
    "plot!(p_combined, range_phase2, losses_lbfgs, \n",
    "      label=\"Phase II: L-BFGS (Fine)\", color=:red, lw=2)\n",
    "\n",
    "# Add a vertical line to mark the hand-off\n",
    "vline!(p_combined, [length(losses_adam)], label=\"Optimizer Switch\", color=:black, linestyle=:dash)\n",
    "\n",
    "display(p_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09636c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
