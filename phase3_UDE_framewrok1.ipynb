{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac7ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the nessecessary packages have been imported\n"
     ]
    }
   ],
   "source": [
    "using SciMLSensitivity\n",
    "using DifferentialEquations\n",
    "using SciMLSensitivity   # or DiffEqSensitivity if you prefer\n",
    "using Zygote\n",
    "using Optimisers         # for optimizer & update\n",
    "using LinearAlgebra\n",
    "using DifferentialEquations\n",
    "using Flux\n",
    "using Plots\n",
    "using Optimization\n",
    "using OptimizationOptimisers\n",
    "using Zygote\n",
    "using DataFrames\n",
    "\n",
    "using Random\n",
    "Random.seed!(1234)\n",
    "println(\"All the nessecessary packages have been imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "452689c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-54.387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Hodgkin-Huxley Model Parameters (Global Constants)\n",
    "\n",
    "\n",
    "# Physical Constants\n",
    "const Cm = 1.0        # ŒºF/cm^2\n",
    "const g_Na = 120.0    # mS/cm^2\n",
    "const g_K = 36.0      # mS/cm^2\n",
    "const g_L = 0.3       # mS/cm^2\n",
    "const E_Na = 50.0     # mV\n",
    "const E_K = -77.0     # mV\n",
    "const E_L = -54.387   # mV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4efd61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics of neural dynamics has been defined\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Known Physics & Stimulus ---\n",
    "\n",
    "# Voltage-gated ion channel kinetics\n",
    "Œ±_n(V) = 0.01 * (V + 55) / (1 - exp(-(V + 55) / 10))\n",
    "Œ≤_n(V) = 0.125 * exp(-(V + 65) / 80)\n",
    "Œ±_m(V) = 0.1 * (V + 40) / (1 - exp(-(V + 40) / 10))\n",
    "Œ≤_m(V) = 4.0 * exp(-(V + 65) / 18)\n",
    "Œ±_h(V) = 0.07 * exp(-(V + 65) / 20)\n",
    "Œ≤_h(V) = 1 / (1 + exp(-(V + 35) / 10))\n",
    "\n",
    "# Steady-state & time-constant functions for the 2D model\n",
    "m_inf(V) = Œ±_m(V) / (Œ±_m(V) + Œ≤_m(V))\n",
    "h_inf(V) = Œ±_h(V) / (Œ±_h(V) + Œ≤_h(V))\n",
    "n_inf(V) = Œ±_n(V) / (Œ±_n(V) + Œ≤_n(V))\n",
    "tau_n(V) = 1 / (Œ±_n(V) + Œ≤_n(V))\n",
    "\n",
    "println(\"Physics of neural dynamics has been defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8ab11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An extra current form neighbour to generate a pulse\n"
     ]
    }
   ],
   "source": [
    "function Stimulus(t)\n",
    "    # A 1ms pulse starting at 10ms\n",
    "    return(t>-10.0 && t<11.0) ? 20 : 0.0\n",
    "end\n",
    "\n",
    "println(\" An extra current form neighbour to generate a pulse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02c1dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>5√ó2 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">t</th><th style = \"text-align: left;\">V</th></tr><tr class = \"columnLabelRow\"><th class = \"stubheadLabel\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">-65.0</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">0.5</td><td style = \"text-align: right;\">-54.674</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">1.0</td><td style = \"text-align: right;\">-30.723</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">-22.2061</td></tr><tr class = \"dataRow\"><td class = \"rowLabel\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: right;\">-31.3453</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& t & V\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.0 & -65.0 \\\\\n",
       "\t2 & 0.5 & -54.674 \\\\\n",
       "\t3 & 1.0 & -30.723 \\\\\n",
       "\t4 & 1.5 & -22.2061 \\\\\n",
       "\t5 & 2.0 & -31.3453 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5√ó2 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m‚îÇ\u001b[1m t       \u001b[0m\u001b[1m V        \u001b[0m\n",
       "\u001b[1m     \u001b[0m‚îÇ\u001b[90m Float64 \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
       "   1 ‚îÇ     0.0  -65.0\n",
       "   2 ‚îÇ     0.5  -54.674\n",
       "   3 ‚îÇ     1.0  -30.723\n",
       "   4 ‚îÇ     1.5  -22.2061\n",
       "   5 ‚îÇ     2.0  -31.3453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Training Data:\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Data Generation ---\n",
    "\n",
    "# 2D Hodgkin-Huxley reduced model engine\n",
    "function hodgkin_huxley_reduced!(du, u, p, t)\n",
    "    V, n = u\n",
    "    I_ext = Stimulus(t)\n",
    "\n",
    "\n",
    "    # Known 2D current\n",
    "    I_Na = g_Na * m_inf(V)^3 * h_inf(V) * (V - E_Na)\n",
    "    I_K  = g_K  * n^4 * (V - E_K)\n",
    "    I_L  = g_L * (V - E_L)\n",
    "    du[1] = (I_ext - I_Na - I_K - I_L) / Cm\n",
    "    du[2] = (n_inf(V) - n) / tau_n(V)\n",
    "end\n",
    "\n",
    "# Generate Data\n",
    "u0_true = [-65.0, n_inf(-65.0)]\n",
    "tspan = (0.0, 50.0)\n",
    "prob_true = ODEProblem(hodgkin_huxley_reduced!, u0_true, tspan)\n",
    "sol_true = solve(prob_true, Rodas5P(), saveat=0.5)\n",
    "\n",
    "# Extract and structure the training data\n",
    "data_V = sol_true[1, :]\n",
    "t_train = sol_true.t\n",
    "\n",
    "# (Optional) Verify data shape and content\n",
    "df = DataFrame(t=t_train, V=data_V )\n",
    "println(\"Generated Training Data:\")\n",
    "display(first(df, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87aba3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(1 => 15, tanh),                 \u001b[90m# 30 parameters\u001b[39m\n",
       "  Dense(15 => 1),                       \u001b[90m# 16 parameters\u001b[39m\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m46 parameters, 392 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "U = Chain(\n",
    "    Dense(1,15, tanh,init = Flux.glorot_uniform),\n",
    "    # Dense(15,30,tanh,init = Flux.glorot_uniform),\n",
    "    Dense(15,1,init=Flux.glorot_uniform)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25341977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recruit Constructed. Parameters: 46\n"
     ]
    }
   ],
   "source": [
    "# Extract the trainable parameters (p_nn) and the re-structuring function (re)\n",
    "p_nn, re = Flux.destructure(U)\n",
    "println(\"Recruit Constructed. Parameters: \", length(p_nn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb12fe",
   "metadata": {},
   "source": [
    "## The hybrid UDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ff65eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Engine Assembled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the UDE function with the embedded neural network\n",
    "function ude_dynamics!(du, u, p, t)\n",
    "    V, n = u\n",
    "    # p --> p_nn neural network parameters\n",
    "    # Neural network component to learn the unknown current\n",
    "    # we will normalize V roughly ( divide by 100) to keep inputs clean for the NN\n",
    "\n",
    "    nn_input = V / 100.0\n",
    "    # We divide V by 100.0 to keep inputs small for the Neural Network\n",
    "    # Example: -65mV becomes -0.65\n",
    "\n",
    "    nn_I_Na = re(p)([nn_input])[1]\n",
    "    # Known physics components\n",
    "    I_ext = Stimulus(t)\n",
    "    I_K  = g_K  * n^4 * (V - E_K)\n",
    "    I_L  = g_L * (V - E_L)\n",
    "    \n",
    "    # The hybrid dynamics equation\n",
    "    du[1] = (I_ext + nn_I_Na - I_K - I_L) / Cm\n",
    "    du[2] = (n_inf(V) - n) / tau_n(V)\n",
    "end\n",
    "println(\"Hybrid Engine Assembled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deaa6f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective Functions Defined.\n"
     ]
    }
   ],
   "source": [
    "# ---- Stable predict function using BacksolveAdjoint and Float64 inputs ----\n",
    "\n",
    "\n",
    "\n",
    "prob_nn = ODEProblem(ude_dynamics!,u0_true,tspan , p_nn)\n",
    "function predict_ude(p)\n",
    "    # build problem with the current flattened NN params\n",
    "\n",
    "\n",
    "    _prob=remake(prob_nn,p=p)\n",
    "    \n",
    "    \n",
    "    solve(_prob, Rodas5P(), saveat=t_train, \n",
    "          sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()))\n",
    "end\n",
    "\n",
    "\n",
    "# ---- Loss function (keep as Float64) ----\n",
    "function loss(p)\n",
    "    pred = predict_ude(p)\n",
    "    if pred.retcode != :Success\n",
    "        return 1e6\n",
    "    end\n",
    "    pred_V = pred[1, :]\n",
    "    loss_val = sum(abs2, pred_V .- data_V)\n",
    "    return loss_val\n",
    "end\n",
    "\n",
    "println(\"Objective Functions Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "149bead0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"neuron_mission_log.jld2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JLD2\n",
    "using OptimizationOptimJL # Required for BFGS/LBFGS\n",
    "\n",
    "# File path for our mission log\n",
    "const CHECKPOINT_FILE = \"neuron_mission_log.jld2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daebbac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_checkpoint_if_exists (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Container to hold all losses in memory for plotting later\n",
    "all_losses = Float64[]\n",
    "\n",
    "# We create a robust callback generator\n",
    "function create_callback(phase_name)\n",
    "    return function (p, l)\n",
    "        # 1. Check for failure (Exploding Gradients)\n",
    "        if isnan(l)\n",
    "            @warn \"!!! ABORT MISSION: Loss is NaN in $phase_name !!!\"\n",
    "            return true # halting the optimization\n",
    "        end\n",
    "        \n",
    "        # 2. Record the loss\n",
    "        push!(all_losses, l)\n",
    "        \n",
    "        # 3. SAVE TO DISK (The Checkpoint)\n",
    "        # We save the current parameters 'p' and the history 'all_losses'\n",
    "        # Saving every iteration on a small model (46 params) is fast and safe.\n",
    "        jldsave(CHECKPOINT_FILE; params=p, loss_history=all_losses)\n",
    "\n",
    "        # 4. Status Report (Every 50 steps)\n",
    "        current_iter = length(all_losses)\n",
    "        if current_iter % 50 == 0\n",
    "            println(\"[$phase_name] Iter: $current_iter | Loss: $l\")\n",
    "        end\n",
    "        \n",
    "        return false # Continue training\n",
    "    end\n",
    "end\n",
    "\n",
    "# Resume Function: Checks if we have previous intel\n",
    "function load_checkpoint_if_exists(initial_params)\n",
    "    if isfile(CHECKPOINT_FILE)\n",
    "        println(\"üìÇ INTEL FOUND: Loading previous checkpoint...\")\n",
    "        data = load(CHECKPOINT_FILE)\n",
    "        # Restore global history\n",
    "        global all_losses = data[\"loss_history\"]\n",
    "        # Return saved parameters\n",
    "        println(\"   -> Resuming from Iteration $(length(all_losses)) with Loss $(last(all_losses))\")\n",
    "        return data[\"params\"]\n",
    "    else\n",
    "        println(\"üåü NO INTEL: Starting fresh recruit training.\")\n",
    "        return initial_params\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b13026e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü NO INTEL: Starting fresh recruit training.\n",
      "\n",
      "--- STAGE 1: ROUGH MANEUVERS (Adam 0.05) ---\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `optf` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `optf` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\Admin\\Downloads\\Neural_Spiking_Dynamics\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:12"
     ]
    }
   ],
   "source": [
    "# --- LOAD STARTING STATE ---\n",
    "# Check if we crashed before. If so, resume from there.\n",
    "# If not, start with fresh recruit parameters (p_nn).\n",
    "current_params = load_checkpoint_if_exists(p_nn)\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 1: ROUGH TRAINING (Adam lr=0.05)\n",
    "# Goal: Quickly find the general shape of the valley\n",
    "# ==========================================\n",
    "if length(all_losses) < 1000\n",
    "    println(\"\\n--- STAGE 1: ROUGH MANEUVERS (Adam 0.05) ---\")\n",
    "    optprob = Optimization.OptimizationProblem(optf, current_params)\n",
    "    \n",
    "    res1 = Optimization.solve(optprob, OptimizationOptimisers.Adam(0.05), \n",
    "                              callback=create_callback(\"Stage1\"), \n",
    "                              maxiters=1000 - length(all_losses)) # Only run what's left\n",
    "    \n",
    "    global current_params = res1.u # Update our best params\n",
    "else\n",
    "    println(\"‚úÖ Stage 1 completed previously.\")\n",
    "end\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 2: FINE TUNING (Adam lr=0.01)\n",
    "# Goal: Settle into the stable orbit (limit cycle/spike)\n",
    "# ==========================================\n",
    "if length(all_losses) < 2000\n",
    "    println(\"\\n--- STAGE 2: FINE TUNING (Adam 0.01) ---\")\n",
    "    optprob2 = Optimization.OptimizationProblem(optf, current_params)\n",
    "    \n",
    "    # Calculate remaining iterations needed to reach 2000\n",
    "    remaining_iters = 2000 - length(all_losses)\n",
    "    \n",
    "    if remaining_iters > 0\n",
    "        res2 = Optimization.solve(optprob2, OptimizationOptimisers.Adam(0.01), \n",
    "                                  callback=create_callback(\"Stage2\"), \n",
    "                                  maxiters=remaining_iters)\n",
    "        global current_params = res2.u\n",
    "    end\n",
    "else\n",
    "    println(\"‚úÖ Stage 2 completed previously.\")\n",
    "end\n",
    "\n",
    "# ==========================================\n",
    "# STAGE 3: FINAL POLISH (L-BFGS)\n",
    "# Goal: Mathematical precision. \n",
    "# Note: L-BFGS requires `OptimizationOptimJL` and can result in NaNs if unstable.\n",
    "# ==========================================\n",
    "println(\"\\n--- STAGE 3: THE POLISH (L-BFGS) ---\")\n",
    "\n",
    "# Checkpointing L-BFGS is harder because it keeps internal memory (Hessian).\n",
    "# We essentially restart L-BFGS from our best point every time we run this block.\n",
    "optprob3 = Optimization.OptimizationProblem(optf, current_params)\n",
    "\n",
    "try\n",
    "    res3 = Optimization.solve(optprob3, OptimizationOptimJL.LBFGS(), \n",
    "                              callback=create_callback(\"Stage3_LBFGS\"), \n",
    "                              maxiters=500) # Run for 500 steps of polishing\n",
    "    global current_params = res3.u\n",
    "catch e\n",
    "    println(\"‚ö†Ô∏è L-BFGS encountered instability or interrupt. Parameters saved safely.\")\n",
    "    println(\"Error: \", e)\n",
    "end\n",
    "\n",
    "println(\"\\nüéâ MISSION COMPLETE. Total Iterations: $(length(all_losses))\")\n",
    "println(\"   Final Saved Loss: $(last(all_losses))\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
